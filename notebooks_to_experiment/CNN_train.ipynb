{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87715326-0b6c-4cab-8145-0dce95109be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import sklearn\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import ecg_plot\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e3f3ef-bc58-417f-a1a4-d792fe31be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waves_from_file(filepath):\n",
    "    xpath = '//RestingECGMeasurements/MedianSamples/WaveformData' #- contains the waveform data of the ecg strip for the given lead\n",
    "    try:\n",
    "        df = pd.read_xml(filepath, xpath=xpath)\n",
    "    except ValueError as value_error:\n",
    "        print('file_name', filepath, 'error', value_error)\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print('file_name', filepath, 'error', e)\n",
    "        return []\n",
    "    waves = get_waveform(df)\n",
    "    normalized_waves = normalize_waveform(waves)\n",
    "    return normalized_waves\n",
    "\n",
    "def get_waveform(df, column='WaveformData'):\n",
    "    waves = df[column]\n",
    "    waves_processed = []\n",
    "    for wave in waves:\n",
    "        wave = re.sub(r\"\\s+\", \"\", wave)\n",
    "        res = [int(num) for num in wave.split(',')]\n",
    "        waves_processed.append(list(res))\n",
    "    return waves_processed\n",
    "\n",
    "def normalize_waveform(waves):\n",
    "    scaler = MinMaxScaler((-1, 1))\n",
    "    return [scaler.fit_transform(np.array(wave).reshape(-1, 1)) for wave in waves]\n",
    "\n",
    "def get_raw_diagnosis(filepath):\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    diagnosis = ''\n",
    "    for inter in root.findall('Interpretation'):\n",
    "        obj = inter.find('Diagnosis')\n",
    "        if obj != None:\n",
    "            for diag in obj:\n",
    "                diagnosis += diag.text + '#'\n",
    "    return diagnosis[:-1]\n",
    "    \n",
    "def get_dataset():\n",
    "    skipped_files = 0\n",
    "    normal_label = 'Normal ECG'\n",
    "    dir = '/groups/umcg-endocrinology/tmp02/projects/ukb-55495/data/metaData/v5/xml_T3/'\n",
    "    X, data_labels = [], []\n",
    "    for file in sorted(os.listdir(dir)):\n",
    "        file_name = os.path.join(dir, file)\n",
    "\n",
    "        # get waves (X)\n",
    "        normalized_12_waves = np.array(get_waves_from_file(file_name)).ravel()\n",
    "        if len(normalized_12_waves) == 0:\n",
    "            skipped_files += 1\n",
    "            continue\n",
    "        else:\n",
    "            X.append(normalized_12_waves)\n",
    "\n",
    "        # get labels (y)\n",
    "        diagnosis = get_raw_diagnosis(file_name)\n",
    "        reversed_diagnosis = diagnosis[::-1]\n",
    "        position = reversed_diagnosis.find('#')\n",
    "        new_diagnosis = reversed_diagnosis[position+1:][::-1]\n",
    "        if normal_label in new_diagnosis:\n",
    "            data_labels.append(0)\n",
    "        else:\n",
    "            data_labels.append(1)\n",
    "    print(\"skipped\", skipped_files, \"files\")\n",
    "    return X, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295868f6-b22c-4d72-b19d-de286eb59462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def calculate_class_ratio(labels):\n",
    "    normals, abnormals = 0, 0\n",
    "    for label in labels:\n",
    "        if label == 0:\n",
    "            normals += 1\n",
    "        else:\n",
    "            abnormals += 1\n",
    "\n",
    "    ratio = (abnormals // normals)\n",
    "    print(\"normals:\", normals, \"abnormals:\", abnormals)\n",
    "    print(\"normals_to_abnormals_ration:\", ratio)\n",
    "    return ratio\n",
    "\n",
    "def solve_imbalance_problem(X, labels):\n",
    "\n",
    "    X = [np.append(X[i], labels[i]) for i in range(len(X))]\n",
    "        \n",
    "    ratio = calculate_class_ratio(labels)\n",
    "\n",
    "    x_new = []\n",
    "    for x in X:\n",
    "        if x[-1] == 0:\n",
    "            for i in range(ratio):\n",
    "                x_new.append(x)\n",
    "\n",
    "    for xx in x_new:\n",
    "        X.append(xx)\n",
    "    random.shuffle(X)\n",
    "\n",
    "    updated_labels = []\n",
    "\n",
    "    # separate waves and labels again\n",
    "    for i in range(len(X)):\n",
    "        updated_labels.append(X[i][-1])\n",
    "        X[i] = X[i][:-1]\n",
    "    \n",
    "    calculate_class_ratio(updated_labels)\n",
    "\n",
    "    return np.array(X), np.array(updated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d1bfe5-fe4b-44ba-ba3a-22af50648087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "def reverse_one_hot(predictions):\n",
    "    reversed_x = []\n",
    "    for x in predictions:\n",
    "        reversed_x.append(np.argmax(np.array(x)))\n",
    "    return reversed_x\n",
    "\n",
    "image_shape = (24, 300)\n",
    "# image_shape = (12, 600)\n",
    "# image_shape = (1, 600*12) # does not work\n",
    "\n",
    "def learn_the_model(training_set_X, training_set_y, testing_set_X, testing_set_y, image_shape = (24, 300), num_classes = 2):\n",
    "\n",
    "    test_labels = tf.keras.utils.to_categorical(testing_set_y, num_classes)\n",
    "    train_labels = tf.keras.utils.to_categorical(training_set_y, num_classes)\n",
    "    \n",
    "    train_images = training_set_X.reshape(training_set_X.shape[0], image_shape[0], image_shape[1], 1)\n",
    "    test_images = testing_set_X.reshape(testing_set_X.shape[0], image_shape[0], image_shape[1], 1)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_shape[0], image_shape[1], 1)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adadelta(), metrics=['accuracy', 'mae', 'mse'])\n",
    "    \n",
    "    train_data_size = train_images.shape[0]\n",
    "    test_data_size = test_images.shape[0]\n",
    "    \n",
    "    print(\"model will be trained with {} and be tested with {} sample\".format(train_data_size, test_data_size))\n",
    "    print(\"Fitting model to the training data...\")\n",
    "    model.fit(train_images, train_labels, batch_size=150, epochs=20, verbose=1, validation_data=None)\n",
    "    \n",
    "    predictions_test = model.predict(test_images, batch_size=150, verbose=1)\n",
    "    predictions_train = model.predict(train_images, batch_size=150, verbose=1)\n",
    "    print(model.evaluate(test_images, test_labels, batch_size=150, verbose=1))\n",
    "    return predictions_test, predictions_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ba317e-0ba7-45e2-8fdc-79c97e949262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision_recall_fscore_support(testing_set_y, reverse_one_hot(predictions_replicate))\n",
    "import seaborn as sns\n",
    "\n",
    "def depict_confusion_matrix(cf_matrix, title):\n",
    "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "    group_counts = ['{0:0.0f}'.format(value) for value in\n",
    "                    cf_matrix.flatten()]\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in\n",
    "                         cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n",
    "              zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8464daff-0f6f-40ca-8315-b9832a4791e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_dataset() # 0 - normal ECG, 1 - abnormal ECG\n",
    "X, y = np.array(X), np.array(y)   # 500 - number of samples per second\n",
    "print(X.shape, y.shape)          # 2439 patients, 7200 = 12 leads * 600 measurements in time (600 - number of samples of one lead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad516d36-aa62-46f9-a10e-ffde5da667a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_proportion = int(0.75 * len(X))\n",
    "\n",
    "training_set_X = X[:training_proportion]\n",
    "testing_set_X = X[training_proportion:]\n",
    "training_set_y = y[:training_proportion]\n",
    "testing_set_y = y[training_proportion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1b81d-8d74-47c5-aabc-a9c481575751",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# oversampling - replicating minority class intances\n",
    "X_train_replicate, y_train_replicate = solve_imbalance_problem(training_set_X, training_set_y)\n",
    "\n",
    "print('Original dataset shape:', Counter(training_set_y))\n",
    "print('Resampled dataset shape:', Counter(y_train_replicate))\n",
    "\n",
    "predictions_replicate_test, predictions_replicate_train = learn_the_model(X_train_replicate, y_train_replicate, testing_set_X, testing_set_y)\n",
    "print(\"Evaluation accuracy score (test) = \", accuracy_score(testing_set_y, reverse_one_hot(predictions_replicate_test)))\n",
    "print(\"Evaluation accuracy score (train) = \", accuracy_score(y_train_replicate, reverse_one_hot(predictions_replicate_train)))\n",
    "\n",
    "# print(precision_recall_fscore_support(testing_set_y, reverse_one_hot(predictions_replicate)))\n",
    "\n",
    "cf_matrix_test = confusion_matrix(testing_set_y, reverse_one_hot(predictions_replicate_test))\n",
    "cf_matrix_train = confusion_matrix(y_train_replicate, reverse_one_hot(predictions_replicate_train))\n",
    "\n",
    "depict_confusion_matrix(cf_matrix_test, 'Testing set')\n",
    "depict_confusion_matrix(cf_matrix_train, 'Training set')\n",
    "\n",
    "# 0.43278688524590164\n",
    "# Confusion matrix =  [[119 (TP)  18 (FP)]\n",
    "#  [328 (TN) 145 (FN)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c63ac8c-e2d9-4e65-8565-329b850567e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "\n",
    "# oversampling technique - creates synthetic copies\n",
    "# SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples \n",
    "# in the feature space and drawing a new sample at a point along that line.\n",
    "sm = SMOTE(random_state=40, n_jobs=8, sampling_strategy='not majority')\n",
    "X_train_resampled, y_train_resampled = sm.fit_resample(training_set_X, training_set_y)\n",
    "\n",
    "print('Original dataset shape:', Counter(training_set_y))\n",
    "print('Resampled dataset shape:', Counter(y_train_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee93cd3-4e1e-4da2-a9f7-9e9f623aa89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_SMOTE_test, predictions_SMOTE_train = learn_the_model(X_train_resampled, y_train_resampled, testing_set_X, testing_set_y)\n",
    "\n",
    "print(\"Evaluation accuracy score (test) = \", accuracy_score(testing_set_y, reverse_one_hot(predictions_SMOTE_test)))\n",
    "print(\"Evaluation accuracy score (train) = \", accuracy_score(y_train_resampled, reverse_one_hot(predictions_SMOTE_train)))\n",
    "\n",
    "cf_matrix_test = confusion_matrix(testing_set_y, reverse_one_hot(predictions_SMOTE_test))\n",
    "depict_confusion_matrix(cf_matrix_test, 'Testing set')\n",
    "\n",
    "cf_matrix_train = confusion_matrix(y_train_resampled, reverse_one_hot(predictions_SMOTE_train))\n",
    "depict_confusion_matrix(cf_matrix_train, 'Training set')\n",
    "\n",
    "# 0.49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185383fc-57b7-415c-84f5-1e2f5eca89ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "from collections import Counter\n",
    "\n",
    "# oversampling technique - adaptive syntethic sampling\n",
    "# generate more synthetic examples in regions of the feature space where the density of minority examples is low, \n",
    "# and fewer or none where the density is high.\n",
    "sm = ADASYN(random_state=40, n_jobs=8, sampling_strategy='not majority')\n",
    "X_train_resampled_ADASYN, y_train_resampled_ADASYN = sm.fit_resample(training_set_X, training_set_y)\n",
    "\n",
    "print('Original dataset shape:', Counter(training_set_y))\n",
    "print('Resampled dataset shape:', Counter(y_train_resampled_ADASYN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a6b62d-6051-4d69-87d1-c10fdbe4dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_ADASYN_test, predictions_ADASYN_train = learn_the_model(X_train_resampled_ADASYN, y_train_resampled_ADASYN, testing_set_X, testing_set_y)\n",
    "print(\"Evaluation accuracy score (test) = \", accuracy_score(testing_set_y, reverse_one_hot(predictions_ADASYN_test)))\n",
    "print(\"Evaluation accuracy score (train) = \", accuracy_score(y_train_resampled_ADASYN, reverse_one_hot(predictions_ADASYN_train)))\n",
    "\n",
    "cf_matrix = confusion_matrix(testing_set_y, reverse_one_hot(predictions_ADASYN_test))\n",
    "depict_confusion_matrix(cf_matrix, 'Testing set')\n",
    "\n",
    "cf_matrix = confusion_matrix(y_train_resampled_ADASYN, reverse_one_hot(predictions_ADASYN_train))\n",
    "depict_confusion_matrix(cf_matrix, 'Training set')\n",
    "\n",
    "# Evaluation accuracy score =  0.578688524590164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a96e53-f766-4d13-89a3-9f0404b514e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "# undersampling technique\n",
    "cc = ClusterCentroids(\n",
    "    estimator=MiniBatchKMeans(n_init=1, random_state=0), random_state=42, sampling_strategy='not minority'\n",
    ")\n",
    "X_train_resampled_CC, y_train_resampled_CC = cc.fit_resample(training_set_X, training_set_y)\n",
    "print('Original dataset shape:', Counter(training_set_y))\n",
    "print('Resampled dataset shape:', Counter(y_train_resampled_CC))\n",
    "# Evaluation accuracy score =  0.3885245901639344\n",
    "# Confusion matrix =  [[112  25]\n",
    "#  [348 125]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3428142-ad36-4ae3-955d-4e3f1bbdc929",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_CC = learn_the_model(X_train_resampled_CC, y_train_resampled_CC, testing_set_X, testing_set_y)\n",
    "print(\"Evaluation accuracy score = \", accuracy_score(testing_set_y, reverse_one_hot(predictions_CC)))\n",
    "\n",
    "cf_matrix = confusion_matrix(testing_set_y, reverse_one_hot(predictions_CC))\n",
    "depict_confusion_matrix(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b8ccde-8943-45af-90c4-d13a892a16d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal \n",
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "# Plot spectrogram \n",
    "fig, ax = plt.subplots()  \n",
    "f, t, Sxx = signal.spectrogram(training_set_X[0], fs=500)\n",
    "print(Sxx.shape, training_set_X[0].shape)\n",
    "pc = ax.pcolormesh(t, f, Sxx, norm=mpl.colors.LogNorm(vmin=Sxx.min(), vmax=Sxx.max()), cmap='inferno') \n",
    "ax.set_ylabel('Frequency') \n",
    "ax.set_xlabel('Time')  \n",
    "fig.colorbar(pc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804937de-3725-4b0e-83f9-2f05e96bc2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectograms(data, sampling_frequency):\n",
    "    specs = [signal.spectrogram(wave, fs=sampling_frequency) for wave in data]\n",
    "    return np.array([spec[2] for spec in specs])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b38646-a1d4-42fa-a13d-d24e9745d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first turn 1d waves into 2d spectograms and after that oversample if possible\n",
    "\n",
    "X_train_specs = create_spectograms(training_set_X, 5)\n",
    "X_test_specs = create_spectograms(testing_set_X, 5)\n",
    "\n",
    "sm = ADASYN(random_state=40, n_jobs=8, sampling_strategy='not majority')\n",
    "print(X_train_specs.shape)\n",
    "X_train_specs = np.array(X_train_specs).reshape(1829, 129*32)\n",
    "print(X_train_specs.shape)\n",
    "\n",
    "X_train_resampled_ADASYN, y_train_resampled_ADASYN = sm.fit_resample(X_train_specs, training_set_y)\n",
    "\n",
    "print(X_train_resampled_ADASYN.shape)\n",
    "X_train_resampled_ADASYN = np.array(X_train_resampled_ADASYN).reshape(X_train_resampled_ADASYN.shape[0], 129, 32)\n",
    "print(X_train_resampled_ADASYN.shape)\n",
    "\n",
    "print('Original dataset shape:', Counter(training_set_y))\n",
    "print('Resampled dataset shape:', Counter(y_train_resampled_ADASYN))\n",
    "\n",
    "predictions_specs_test, predictions_specs_train = learn_the_model(X_train_resampled_ADASYN, y_train_resampled_ADASYN, X_test_specs, testing_set_y, image_shape=(129, 32))\n",
    "\n",
    "print(\"Evaluation accuracy score (test) = \", accuracy_score(testing_set_y, reverse_one_hot(predictions_specs_test)))\n",
    "print(\"Evaluation accuracy score (train)= \", accuracy_score(y_train_resampled_ADASYN, reverse_one_hot(predictions_specs_train)))\n",
    "\n",
    "\n",
    "cf_matrix = confusion_matrix(testing_set_y, reverse_one_hot(predictions_specs_test))\n",
    "depict_confusion_matrix(cf_matrix, 'Testing set')\n",
    "\n",
    "cf_matrix = confusion_matrix(y_train_resampled_ADASYN, reverse_one_hot(predictions_specs_train))\n",
    "depict_confusion_matrix(cf_matrix, 'Training set')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6b19cc-0ca0-4fa1-9f88-2e3f171cf2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# takes 2 minutes to read all three datasets\n",
    "def read_data_from_file(filename):\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',', quoting = csv.QUOTE_NONNUMERIC)\n",
    "        data = [row for row in spamreader]\n",
    "    return np.array(data)\n",
    "\n",
    "waves = read_data_from_file('waves_full.csv')\n",
    "print(waves.shape, waves[0])\n",
    "\n",
    "labels2classes = read_data_from_file('labels_full_2_classes.csv')[0]\n",
    "print(labels2classes.shape)\n",
    "\n",
    "labels3classes = read_data_from_file('labels_full_3_classes.csv')[0]\n",
    "print(labels2classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea65d07a-3100-4e50-92d3-e2606b5ca329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# create frequency map\n",
    "def get_raw_diagnosis(filepath):\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    diagnosis = ''\n",
    "    for inter in root.findall('Interpretation'):\n",
    "        obj = inter.find('Diagnosis')\n",
    "        if obj != None:\n",
    "            for diag in obj:\n",
    "                diagnosis += diag.text + '#'\n",
    "    return diagnosis[:-1]\n",
    "\n",
    "def get_diagnosis_by_label():\n",
    "    dirs = ['/groups/umcg-endocrinology/tmp02/projects/ukb-55495/data/metaData/v5/xml_T3/', \"/groups/umcg-endocrinology/tmp02/projects/ukb-55495/data/metaData/v5/xml_T2/\"]\n",
    "    short_label = []\n",
    "    for dir in dirs:\n",
    "        for file in os.listdir(dir):\n",
    "            file_name = os.path.join(dir, file)\n",
    "            try:\n",
    "                d = get_raw_diagnosis(file_name)\n",
    "            except Exception as e:\n",
    "                print('error', e, 'in', file_name)\n",
    "                continue\n",
    "            reversed_d = d[::-1]\n",
    "            position = reversed_d.find('#')\n",
    "            new_d = reversed_d[position+1:][::-1]\n",
    "            short_label.append(new_d)\n",
    "    return set(short_label), len(short_label), short_label\n",
    "    \n",
    "diags_normal, len_origin, origin = get_diagnosis_by_label()\n",
    "with open('diagnosis_frequencies.csv', 'w') as f:\n",
    "        mywriter = csv.writer(f, delimiter=',')\n",
    "        mywriter.writerow(['Diagnosis', 'Frequency'])\n",
    "        for pair in Counter(origin).most_common(200000):\n",
    "            mywriter.writerow([pair[0], pair[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b8cc7-8ca0-400c-a05c-4cda165c1d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "inertias = []\n",
    "for i in range(1,11):\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "    kmeans.fit(waves)\n",
    "    inertias.append(kmeans.inertia_)inertias\n",
    "\n",
    "plt.plot(range(1,11), inertias, marker='o')\n",
    "plt.title('Elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b300b323-3b63-4846-b90e-188f4a156ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# calculate_class_ratio(labels2classes)\n",
    "X_train, X_test, y_train, y_test = train_test_split(waves, labels2classes, train_size=0.75, stratify=labels2classes)\n",
    "\n",
    "print('Original dataset shape (full):', Counter(labels2classes))\n",
    "print('Resampled dataset shape (full):', Counter(y_train))\n",
    "print('Resampled dataset shape (full):', Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10610ebf-6fe2-4145-a19a-8710288ccc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_class_ratio(labels3classes)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(waves, labels3classes, train_size=0.75, stratify=labels3classes)\n",
    "\n",
    "print('Original dataset shape (full):', Counter(labels3classes))\n",
    "print('Resampled dataset shape (full):', Counter(y_train3))\n",
    "print('Resampled dataset shape (full):', Counter(y_test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b51c3f0-3294-4596-a9fd-e2026e916858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from collections import Counter\n",
    "\n",
    "# undersampling technique - 2 classes\n",
    "\n",
    "cc = ClusterCentroids(\n",
    "    estimator=MiniBatchKMeans(n_init=3, random_state=0), random_state=42, sampling_strategy='not minority'\n",
    ")\n",
    "st = time.time()\n",
    "X_resampled_train, y_resampled_train = cc.fit_resample(X_train, y_train)\n",
    "elapsed_time = time.time() - st\n",
    "print('Undersampling time (full):', elapsed_time/60, 'minutes')\n",
    "\n",
    "print('Original dataset shape (full):', Counter(y_train))\n",
    "print('Resampled dataset shape (full):', Counter(y_resampled_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069d960d-8e5b-452d-b5b7-7855817b52d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from collections import Counter\n",
    "\n",
    "# undersampling technique - 3 classes\n",
    "\n",
    "cc = ClusterCentroids(\n",
    "    estimator=MiniBatchKMeans(n_init=3, random_state=0), random_state=42, sampling_strategy='not minority'\n",
    ")\n",
    "st = time.time()\n",
    "X_resampled_train3, y_resampled_train3 = cc.fit_resample(X_train3, y_train3)\n",
    "elapsed_time = time.time() - st\n",
    "print('Undersampling time (full):', elapsed_time/60, 'minutes')\n",
    "\n",
    "print('Original dataset shape (full):', Counter(y_train3))\n",
    "print('Resampled dataset shape (full):', Counter(y_resampled_train3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4710ee-a0f5-433a-929d-c198d8097d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def learn_the_model_experiment(training_set_X, training_set_y, testing_set_X, testing_set_y, image_shape = (24, 300), num_classes = 2):\n",
    "    batch_size = 100\n",
    "    test_labels = tf.keras.utils.to_categorical(testing_set_y, num_classes)\n",
    "    train_labels = tf.keras.utils.to_categorical(training_set_y, num_classes)\n",
    "    \n",
    "    train_images = training_set_X.reshape(training_set_X.shape[0], image_shape[0], image_shape[1], 1)\n",
    "    test_images = testing_set_X.reshape(testing_set_X.shape[0], image_shape[0], image_shape[1], 1)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(image_shape[0], image_shape[1], 1)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adadelta(), metrics=['accuracy', 'mae', 'mse'])\n",
    "    \n",
    "    train_data_size = train_images.shape[0]\n",
    "    test_data_size = test_images.shape[0]\n",
    "    \n",
    "    print(\"model will be trained with {} and be tested with {} sample\".format(train_data_size, test_data_size))\n",
    "    print(\"Fitting model to the training data...\")\n",
    "    model.fit(train_images, train_labels, batch_size=batch_size, epochs=150, verbose=1, validation_data=None)\n",
    "    \n",
    "    predictions_test = model.predict(test_images, batch_size=batch_size, verbose=1)\n",
    "    predictions_train = model.predict(train_images, batch_size=batch_size, verbose=1)\n",
    "    print(model.evaluate(test_images, test_labels, batch_size=batch_size, verbose=1))\n",
    "    return predictions_test, predictions_train\n",
    "\n",
    "def learn_and_test(X_resampled_train3, y_resampled_train3, X_test3, y_test3, num_classes=2):\n",
    "    X_resampled_train3, y_resampled_train3 = shuffle(X_resampled_train3, y_resampled_train3)\n",
    "\n",
    "    st = time.time()\n",
    "    predictions_full_CC_test3, predictions_full_CC_train3 = learn_the_model_experiment(X_resampled_train3, y_resampled_train3, X_test3, y_test3, num_classes=num_classes)\n",
    "\n",
    "    elapsed_time = time.time() - st\n",
    "    print('Training model time (full):', elapsed_time/60, 'minutes')\n",
    "\n",
    "    print(\"Evaluation accuracy score (full, test) = \", accuracy_score(y_test3, reverse_one_hot(predictions_full_CC_test3)))\n",
    "    print(\"Evaluation accuracy score (full, train) = \", accuracy_score(y_resampled_train3, reverse_one_hot(predictions_full_CC_train3)))\n",
    "    \n",
    "    cf_matrix3 = confusion_matrix(y_test3, reverse_one_hot(predictions_full_CC_test3))\n",
    "    if num_classes == 2:\n",
    "        depict_confusion_matrix(cf_matrix3, 'Testing set (full)')\n",
    "    else:\n",
    "        print(cf_matrix3)\n",
    "    \n",
    "    cf_matrix4 = confusion_matrix(y_resampled_train3, reverse_one_hot(predictions_full_CC_train3))\n",
    "    if num_classes == 2:\n",
    "        depict_confusion_matrix(cf_matrix4, 'Training set (full)')\n",
    "    else:\n",
    "        print(cf_matrix4)\n",
    "    return predictions_full_CC_test3, predictions_full_CC_train3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae54b5e-e591-4fce-85a6-963532c7853b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from collections import Counter\n",
    "\n",
    "pred_test, pred_train = learn_and_test(X_resampled_train, y_resampled_train, X_test, y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76461732-5d89-454f-9656-62581d5422b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix3 = confusion_matrix(y_test, reverse_one_hot(pred_test))\n",
    "depict_confusion_matrix(cf_matrix3, 'Testing set (full)')\n",
    "\n",
    "cf_matrix4 = confusion_matrix(y_resampled_train, reverse_one_hot(pred_train))\n",
    "depict_confusion_matrix(cf_matrix4, 'Training set (full)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad58a7c-3e25-4a3c-8c37-1a5ad93974d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "pred_test3, pred_train3 = learn_and_test(X_resampled_train3, y_resampled_train3, X_test, y_test, num_classes=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a11bbe-43cd-4d56-9372-b8cf8c1e4d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original dataset shape (full):', Counter(y_resampled_train3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c428f2-aabc-4c48-b54f-6527015230b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, len(y_resampled_train3[:3000])) \n",
    "# setting the corresponding y - coordinates \n",
    "y = y_resampled_train3[:100]\n",
    "  \n",
    "# plotting the points \n",
    "plt.plot(y) \n",
    "  \n",
    "# function to show the plot \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e271a1b-0c8d-4f55-a931-21e6f3a115e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[:10], y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360392b2-ab2c-4383-ba6e-5bcf132c4960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "# oversampling technique - adaptive syntethic sampling\n",
    "# generate more synthetic examples in regions of the feature space where the density of minority examples is low, \n",
    "# and fewer or none where the density is high.\n",
    "st = time.time()\n",
    "sm = ADASYN(random_state=40, n_jobs=8, sampling_strategy='not majority')\n",
    "X_train_resampled_ADASYN, y_train_resampled_ADASYN = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "elapsed_time = time.time() - st\n",
    "print('ADASYN time (full):', elapsed_time/60, 'minutes')\n",
    "\n",
    "print('Original dataset shape:', Counter(y_train))\n",
    "print('Resampled dataset shape:', Counter(y_train_resampled_ADASYN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd46f7fc-38b2-4e86-8975-0d04eca9aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "from collections import Counter\n",
    "\n",
    "st = time.time()\n",
    "sm = ADASYN(random_state=40, n_jobs=8, sampling_strategy='minority')\n",
    "X_train_resampled_ADASYN3, y_train_resampled_ADASYN3 = sm.fit_resample(X_train3, y_train3)\n",
    "X_train_resampled_ADASYN3, y_train_resampled_ADASYN3 = sm.fit_resample(X_train_resampled_ADASYN3, y_train_resampled_ADASYN3)\n",
    "\n",
    "elapsed_time = time.time() - st\n",
    "print('ADASYN time (full):', elapsed_time/60, 'minutes')\n",
    "\n",
    "print('Original dataset shape:', Counter(y_train3))\n",
    "print('Resampled dataset shape:', Counter(y_train_resampled_ADASYN3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caafe579-193a-47f6-9c59-b1920525e314",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_test_ad2, pred_train_ad2 = learn_and_test(X_train_resampled_ADASYN, y_train_resampled_ADASYN, X_test, y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f93b9da-5782-44d0-ae74-61b7b80e0a87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_test_ad3, pred_train_ad3 = learn_and_test(X_train_resampled_ADASYN3, y_train_resampled_ADASYN3, X_test3, y_test3, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab5889-d33b-4f37-ba0b-3adcef985940",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = [[ 990, 838, 1048],\n",
    " [1511, 4255, 2369],\n",
    " [0,0, 0]]\n",
    "mat2 = [[1178, 1100, 1134],\n",
    " [ 414, 2306,  692],\n",
    " [ 586,  938, 1888]]\n",
    "\n",
    "mat3 = [[2456, 1190, 1806],\n",
    " [ 727, 2906,  789],\n",
    " [ 279,  240,  618]]\n",
    "mat4 = [[ 7321,  3651,  5382],\n",
    " [ 3061, 10102,  3764],\n",
    " [ 3972,  3434, 10208]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c02655-448e-4a1e-8cec-7914af811459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(5,4))\n",
    "sns.heatmap(mat4, annot=True, fmt='g')\n",
    "plt.title('Confusion Matrix (Training data)')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232ea35e-95bd-41b1-8c9b-a6a51f4579c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
